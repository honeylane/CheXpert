{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18157354",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf44b01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "%load_ext tensorboard\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters, Tuner\n",
    "from keras_tuner import Objective\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab04b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcdcbb",
   "metadata": {},
   "source": [
    "## Mirrored Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c49703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "# Define the MirroredStrategy\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5bf28",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4156be07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 64540\n",
      "There are 200\n",
      "There are 500\n"
     ]
    }
   ],
   "source": [
    "source_path = '.'\n",
    "train_directory = os.path.join(source_path, 'CheXpert-v1.0/train')\n",
    "validation_directory = os.path.join(source_path, 'CheXpert-v1.0/valid')\n",
    "test_directory = os.path.join(source_path, 'CheXpert-v1.0/test')\n",
    "\n",
    "print(f\"There are {len(os.listdir(train_directory))}\")\n",
    "print(f\"There are {len(os.listdir(validation_directory))}\")\n",
    "print(f\"There are {len(os.listdir(test_directory))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8464c94a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load train and valid labels\n",
    "train_df = pd.read_csv(os.path.join(source_path, 'CheXpert-v1.0/train.csv'))\n",
    "valid_df = pd.read_csv(os.path.join(source_path, 'CheXpert-v1.0/valid.csv'))\n",
    "test_df = pd.read_csv(os.path.join(source_path, 'CheXpert-v1.0/test.csv'))\n",
    "\n",
    "# Load training and validation image paths\n",
    "train_image_paths = [source_path + '/' + path for path in train_df['Path']]\n",
    "valid_image_paths = [source_path + '/' + path for path in valid_df['Path']]\n",
    "test_image_paths = [source_path + '/' + path for path in test_df['Path']]\n",
    "\n",
    "# Create TensorFlow tensors from image paths\n",
    "train_image_paths = tf.constant(train_image_paths)\n",
    "valid_image_paths = tf.constant(valid_image_paths)\n",
    "test_image_paths = tf.constant(test_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e452a55a",
   "metadata": {},
   "source": [
    "# Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006e3aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df[['Atelectasis','Cardiomegaly','Consolidation','Edema','Pleural Effusion',\n",
    "                     'No Finding','Enlarged Cardiomediastinum', 'Lung Opacity','Lung Lesion','Pneumonia',\n",
    "                     'Pneumothorax', 'Pleural Other', 'Fracture','Support Devices']]\n",
    "# print(train_df.head())     # printing first five rows of the file\n",
    "# print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adaea987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_df = valid_df[['Atelectasis','Cardiomegaly','Consolidation','Edema','Pleural Effusion',\n",
    "                     'No Finding','Enlarged Cardiomediastinum', 'Lung Opacity','Lung Lesion','Pneumonia',\n",
    "                     'Pneumothorax', 'Pleural Other', 'Fracture','Support Devices']]\n",
    "# print(valid_df.head())     # printing first five rows of the file\n",
    "# print(valid_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3055216",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[['Atelectasis','Cardiomegaly','Consolidation','Edema','Pleural Effusion',\n",
    "                     'No Finding','Enlarged Cardiomediastinum', 'Lung Opacity','Lung Lesion','Pneumonia',\n",
    "                     'Pneumothorax', 'Pleural Other', 'Fracture','Support Devices']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc894384",
   "metadata": {},
   "source": [
    "# Train and Valid Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a883624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223414\n"
     ]
    }
   ],
   "source": [
    "train_df_UOnes = train_df.replace(-1,1).fillna(0)\n",
    "print(len(train_df_UOnes))\n",
    "train_labels = np.array(train_df_UOnes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d96c5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "valid_df_UOnes = valid_df.fillna(0)\n",
    "print(len(valid_df_UOnes))\n",
    "valid_labels = np.array(valid_df_UOnes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a989f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "test_df_UOnes = test_df.fillna(0)\n",
    "print(len(test_df_UOnes))\n",
    "test_labels = np.array(test_df_UOnes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3aec6",
   "metadata": {},
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4222b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "\n",
    "#training images preprocessing\n",
    "SIZE = 320\n",
    "\n",
    "# Define a custom preprocessing function\n",
    "def preprocess_image(image_path, label):\n",
    "    # Read the image file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode the image from bytes to a tensor\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "#     # Resize the image to a fixed size\n",
    "#     image = tf.image.resize(image, [SIZE, SIZE])\n",
    "    # Normalize pixel values to be in the range [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c776639",
   "metadata": {},
   "source": [
    "# Prepare the data pipeline by setting batch size & buffer size using tf.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8150102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_image_paths, train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_image_paths, valid_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_image_paths, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f56a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Apply preprocessing function to the datasets\n",
    "train_ds = train_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "valid_ds = valid_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c974fb7",
   "metadata": {},
   "source": [
    "# Visualize Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa13492d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Plot a sample of 10 original images\n",
    "# fig, axes = plt.subplots(1, 10, figsize=(16, 15))  # Adjust the figsize as needed\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for i, (image, label) in enumerate(train_ds.take(10)):\n",
    "#     ax = axes[i]\n",
    "#     ax.imshow(image.numpy())  # Select the first image from the batch\n",
    "#     ax.set_axis_off()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16d6d5",
   "metadata": {},
   "source": [
    "# Augementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f99a122c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(\n",
    "        height_factor=(-0.05, -0.15),\n",
    "        width_factor=(-0.05, -0.15)),\n",
    "    layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61a3d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(ds, shuffle=False, augment=False):\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(5000)\n",
    "\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.cache()\n",
    "\n",
    "    if augment:\n",
    "        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                    num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ecbd76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
    "valid_ds = prepare(valid_ds)\n",
    "test_ds = prepare(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e5f1b",
   "metadata": {},
   "source": [
    "# Visualize Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee774a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define a function to plot sample images\n",
    "# def plot_sample_images(dataset, num_samples=10):\n",
    "#     # Create an iterator for the dataset\n",
    "#     iterator = iter(dataset)\n",
    "\n",
    "#     # Get the next batch of images and labels\n",
    "#     sample_images, sample_labels = next(iterator)\n",
    "\n",
    "#     # Plot the sample images\n",
    "#     fig, axes = plt.subplots(1, num_samples, figsize=(16, 15))\n",
    "#     axes = axes.flatten()\n",
    "\n",
    "#     for i in range(num_samples):\n",
    "#         img = sample_images[i]\n",
    "#         ax = axes[i]\n",
    "#         ax.imshow(img.numpy())  # Convert TensorFlow tensor to NumPy array for plotting\n",
    "#         ax.set_axis_off()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize sample images from the training dataset\n",
    "# plot_sample_images(train_ds, num_samples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a43fa",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7378cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    with mirrored_strategy.scope():\n",
    "        model = tf.keras.models.Sequential()\n",
    "        pre_trained_model = tf.keras.applications.densenet.DenseNet121(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(320, 320, 3)\n",
    "        )\n",
    "\n",
    "        model.add(pre_trained_model)\n",
    "        model.add(GlobalAveragePooling2D(input_shape=(1024, 1, 1)))\n",
    "\n",
    "        # Use hyperparameters to define the units for the first Dense layer\n",
    "        dense1_units = hp.Int('dense_1_units', min_value=512, max_value=3072, step=512)\n",
    "        model.add(Dense(dense1_units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate=hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # Use hyperparameters to define the units for the second Dense layer\n",
    "        dense2_units = hp.Int('dense_2_units', min_value=256, max_value=1536, step=256)\n",
    "        model.add(Dense(dense2_units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(rate=hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        # The third Dense layer is the output layer with fixed units\n",
    "        model.add(tf.keras.layers.Dense(units=14, activation='sigmoid'))\n",
    "\n",
    "        # Define hyperparameters for the learning rate\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log')\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy', tf.keras.metrics.AUC(multi_label=True, num_labels=14)])\n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dc26c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=Objective('val_auc', direction =\"max\"),\n",
    "    max_epochs=4,\n",
    "    factor=5,\n",
    "    hyperband_iterations=1,  # number of times to iterate over the hyperband algorithm\n",
    "    directory='kt_hyperband',\n",
    "    project_name='uone_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3037d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback to stop training early after reaching a certain value for the validation loss.\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5024a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [01h 43m 14s]\n",
      "val_auc: 0.7597945332527161\n",
      "\n",
      "Best val_auc So Far: 0.7597945332527161\n",
      "Total elapsed time: 03h 23m 24s\n"
     ]
    }
   ],
   "source": [
    "# Start the hyperparameter tuning process\n",
    "tuner.search(train_ds, epochs=4, validation_data=(test_ds), callbacks=[stop_early], verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab080290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete. Here are the optimal values:\n",
      "Dense1_units: 3072\n",
      "Dense2_units: 256\n",
      "Dropout1: 0.30000000000000004\n",
      "Dropout2: 0.4\n",
      "Learning rate: 2.1149313641035656e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. Here are the optimal values:\n",
    "Dense1_units: {best_hps.get('dense_1_units')}\n",
    "Dense2_units: {best_hps.get('dense_2_units')}\n",
    "Dropout1: {best_hps.get('dropout_1')}\n",
    "Dropout2: {best_hps.get('dropout_2')}\n",
    "Learning rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6150536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "INFO:tensorflow:Collective all_reduce tensors: 372 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 372 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "13964/13964 - 1622s - loss: 0.6175 - binary_accuracy: 0.6916 - auc_1: 0.5544 - val_loss: 0.4114 - val_binary_accuracy: 0.8184 - val_auc_1: 0.6158 - 1622s/epoch - 116ms/step\n",
      "Epoch 2/10\n",
      "13964/13964 - 1439s - loss: 0.4382 - binary_accuracy: 0.8075 - auc_1: 0.6092 - val_loss: 0.3887 - val_binary_accuracy: 0.8358 - val_auc_1: 0.6516 - 1439s/epoch - 103ms/step\n",
      "Epoch 3/10\n",
      "13964/13964 - 1443s - loss: 0.4164 - binary_accuracy: 0.8175 - auc_1: 0.6418 - val_loss: 0.3898 - val_binary_accuracy: 0.8413 - val_auc_1: 0.7214 - 1443s/epoch - 103ms/step\n",
      "Epoch 4/10\n",
      "13964/13964 - 1442s - loss: 0.4027 - binary_accuracy: 0.8241 - auc_1: 0.6647 - val_loss: 0.3965 - val_binary_accuracy: 0.8364 - val_auc_1: 0.6694 - 1442s/epoch - 103ms/step\n",
      "Epoch 5/10\n",
      "13964/13964 - 1446s - loss: 0.3939 - binary_accuracy: 0.8285 - auc_1: 0.6807 - val_loss: 0.4239 - val_binary_accuracy: 0.8214 - val_auc_1: 0.6645 - 1446s/epoch - 104ms/step\n",
      "Epoch 6/10\n",
      "13964/13964 - 1446s - loss: 0.3872 - binary_accuracy: 0.8318 - auc_1: 0.6945 - val_loss: 0.3986 - val_binary_accuracy: 0.8370 - val_auc_1: 0.6521 - 1446s/epoch - 104ms/step\n",
      "Epoch 7/10\n",
      "13964/13964 - 1449s - loss: 0.3826 - binary_accuracy: 0.8343 - auc_1: 0.7042 - val_loss: 0.4372 - val_binary_accuracy: 0.8278 - val_auc_1: 0.6353 - 1449s/epoch - 104ms/step\n",
      "Epoch 8/10\n",
      "13964/13964 - 1476s - loss: 0.3788 - binary_accuracy: 0.8357 - auc_1: 0.7133 - val_loss: 0.4712 - val_binary_accuracy: 0.8138 - val_auc_1: 0.6094 - 1476s/epoch - 106ms/step\n",
      "Epoch 9/10\n",
      "13964/13964 - 1479s - loss: 0.3756 - binary_accuracy: 0.8373 - auc_1: 0.7210 - val_loss: 0.4209 - val_binary_accuracy: 0.8342 - val_auc_1: 0.6816 - 1479s/epoch - 106ms/step\n",
      "Epoch 10/10\n",
      "13964/13964 - 1459s - loss: 0.3733 - binary_accuracy: 0.8385 - auc_1: 0.7267 - val_loss: 0.4254 - val_binary_accuracy: 0.8346 - val_auc_1: 0.6350 - 1459s/epoch - 104ms/step\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "# Find the optimal number of epochs to train the model with the hyperparameters obtained from the search.\n",
    "cnn = tuner.hypermodel.build(best_hps)\n",
    "history = cnn.fit(train_ds, epochs=10, validation_data=(valid_ds), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e47ca79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 3\n"
     ]
    }
   ],
   "source": [
    "val_auc_per_epoch = history.history['val_auc_1']\n",
    "best_epoch = val_auc_per_epoch.index(max(val_auc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96009d5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b7e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout\n",
    "# from tensorflow.keras.applications.densenet import DenseNet121\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def create_model():\n",
    "#     model = tf.keras.models.Sequential()\n",
    "#     pre_trained_model = tf.keras.applications.densenet.DenseNet121(\n",
    "#         include_top=False,\n",
    "#         weights='imagenet',\n",
    "#         input_shape=(320, 320, 3)\n",
    "#     )\n",
    "\n",
    "# #     for layer in pre_trained_model.layers:\n",
    "# #         layer.trainable = False\n",
    "\n",
    "#     model.add(pre_trained_model)\n",
    "#     model.add(GlobalAveragePooling2D(input_shape=(1024, 1, 1)))\n",
    "#     model.add(Dense(2048, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(tf.keras.layers.Dense(units=14, activation='sigmoid'))\n",
    "    \n",
    "# #     model.add(tf.keras.layers.Flatten())\n",
    "# #     model.add(tf.keras.layers.Dense(units = 512, activation = 'relu'))\n",
    "# #     model.add(tf.keras.layers.Dense(units = 5, activation = 'sigmoid'))\n",
    "\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999),\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['binary_accuracy', tf.keras.metrics.AUC(multi_label=True, num_labels=14)])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d461fe",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60480dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def create_callbacks():\n",
    "#     log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#     checkpoint_dir = \"logs/fit/uonescheckpoint\"\n",
    "#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "#     class SaveCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "#         def __init__(self, checkpoint_dir, save_interval):\n",
    "#             super(SaveCheckpointCallback, self).__init__()\n",
    "#             self.checkpoint_dir = checkpoint_dir\n",
    "#             self.save_interval = save_interval\n",
    "#             self.iteration = 0\n",
    "\n",
    "#         def on_batch_end(self, batch, logs=None):\n",
    "#             self.iteration += 1\n",
    "#             if self.iteration % self.save_interval == 0:\n",
    "#                 model_checkpoint = os.path.join(self.checkpoint_dir, f\"model_checkpoint_{self.iteration}.h5\")\n",
    "#                 self.model.save(model_checkpoint)\n",
    "#                 print(f\"Saved checkpoint at iteration {self.iteration} to {model_checkpoint}\")\n",
    "\n",
    "#     save_interval = 4800  # Adjust this as needed\n",
    "#     checkpoint_callback = SaveCheckpointCallback(checkpoint_dir, save_interval)\n",
    "    \n",
    "#     return [checkpoint_callback, tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4133001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def create_callbacks(run_num):\n",
    "    log_dir = \"logs/fit/uones_ht\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Modify the checkpoint_dir to include the run number.\n",
    "    checkpoint_dir = f\"logs/fit/uones_ht/run_{run_num}\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    # SaveCheckpointCallback class definition\n",
    "    class SaveCheckpointCallback(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, checkpoint_dir, save_interval):\n",
    "            super(SaveCheckpointCallback, self).__init__()\n",
    "            self.checkpoint_dir = checkpoint_dir\n",
    "            self.save_interval = save_interval\n",
    "            self.iteration = 0\n",
    "\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.iteration += 1\n",
    "            if self.iteration % self.save_interval == 0:\n",
    "                model_checkpoint = os.path.join(self.checkpoint_dir, f\"model_checkpoint_{self.iteration}.h5\")\n",
    "                self.model.save(model_checkpoint)\n",
    "                print(f\"Saved checkpoint at iteration {self.iteration} to {model_checkpoint}\")\n",
    "\n",
    "    save_interval = 4800 \n",
    "    checkpoint_callback = SaveCheckpointCallback(checkpoint_dir, save_interval)\n",
    "    \n",
    "    return [checkpoint_callback, tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabf9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_models = []\n",
    "\n",
    "# %tensorboard --logdir logs --port 8885\n",
    "\n",
    "# def train(num_runs, train_ds, valid_ds):\n",
    "\n",
    "#     for run in range(num_runs):\n",
    "#         print(f\"Run {run + 1} of {num_runs}\")\n",
    "\n",
    "#         # Clear previous session to ensure a fresh start for each run\n",
    "#         tf.keras.backend.clear_session()\n",
    "\n",
    "#         model = create_model()\n",
    "#         callbacks = create_callbacks(run+1)\n",
    "\n",
    "#         start = time.time()\n",
    "#         history = model.fit(train_ds, epochs=4, validation_data=valid_ds, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
    "#         print(\"Total time for run\", run + 1, \": \", time.time() - start, \"seconds\")\n",
    "        \n",
    "#         trained_models.append(model)\n",
    "\n",
    "#     return trained_models\n",
    "\n",
    "# # Define the number of runs\n",
    "# num_runs = 3\n",
    "# training = train(num_runs, train_ds, valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f08cd1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 of 3\n",
      "Epoch 1/3\n",
      "INFO:tensorflow:Collective all_reduce tensors: 372 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 372 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1230s vs `on_train_batch_end` time: 0.7701s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint at iteration 4800 to logs/fit/uones_ht/run_1/model_checkpoint_4800.h5\n",
      "Saved checkpoint at iteration 9600 to logs/fit/uones_ht/run_1/model_checkpoint_9600.h5\n",
      "13964/13964 - 1674s - loss: 0.6220 - binary_accuracy: 0.6871 - auc: 0.5536 - val_loss: 0.4264 - val_binary_accuracy: 0.8257 - val_auc: 0.6434 - 1674s/epoch - 120ms/step\n",
      "Epoch 2/3\n",
      "Saved checkpoint at iteration 14400 to logs/fit/uones_ht/run_1/model_checkpoint_14400.h5\n",
      "Saved checkpoint at iteration 19200 to logs/fit/uones_ht/run_1/model_checkpoint_19200.h5\n",
      "Saved checkpoint at iteration 24000 to logs/fit/uones_ht/run_1/model_checkpoint_24000.h5\n",
      "13964/13964 - 1525s - loss: 0.4385 - binary_accuracy: 0.8073 - auc: 0.6091 - val_loss: 0.4109 - val_binary_accuracy: 0.8388 - val_auc: 0.6359 - 1525s/epoch - 109ms/step\n",
      "Epoch 3/3\n",
      "Saved checkpoint at iteration 28800 to logs/fit/uones_ht/run_1/model_checkpoint_28800.h5\n",
      "Saved checkpoint at iteration 33600 to logs/fit/uones_ht/run_1/model_checkpoint_33600.h5\n",
      "Saved checkpoint at iteration 38400 to logs/fit/uones_ht/run_1/model_checkpoint_38400.h5\n",
      "13964/13964 - 1494s - loss: 0.4163 - binary_accuracy: 0.8176 - auc: 0.6421 - val_loss: 0.4055 - val_binary_accuracy: 0.8422 - val_auc: 0.6987 - 1494s/epoch - 107ms/step\n",
      "Total time for run 1 :  4695.712975978851 seconds\n",
      "Run 2 of 3\n",
      "Epoch 1/3\n",
      "INFO:tensorflow:Collective all_reduce tensors: 372 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 372 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1196s vs `on_train_batch_end` time: 0.7739s). Check your callbacks.\n",
      "Saved checkpoint at iteration 4800 to logs/fit/uones_ht/run_2/model_checkpoint_4800.h5\n",
      "Saved checkpoint at iteration 9600 to logs/fit/uones_ht/run_2/model_checkpoint_9600.h5\n",
      "13964/13964 - 1718s - loss: 0.6155 - binary_accuracy: 0.6917 - auc: 0.5550 - val_loss: 0.4056 - val_binary_accuracy: 0.8294 - val_auc: 0.5967 - 1718s/epoch - 123ms/step\n",
      "Epoch 2/3\n",
      "Saved checkpoint at iteration 14400 to logs/fit/uones_ht/run_2/model_checkpoint_14400.h5\n",
      "Saved checkpoint at iteration 19200 to logs/fit/uones_ht/run_2/model_checkpoint_19200.h5\n",
      "Saved checkpoint at iteration 24000 to logs/fit/uones_ht/run_2/model_checkpoint_24000.h5\n",
      "13964/13964 - 1578s - loss: 0.4383 - binary_accuracy: 0.8076 - auc: 0.6091 - val_loss: 0.4291 - val_binary_accuracy: 0.8333 - val_auc: 0.6203 - 1578s/epoch - 113ms/step\n",
      "Epoch 3/3\n",
      "Saved checkpoint at iteration 28800 to logs/fit/uones_ht/run_2/model_checkpoint_28800.h5\n",
      "Saved checkpoint at iteration 33600 to logs/fit/uones_ht/run_2/model_checkpoint_33600.h5\n",
      "Saved checkpoint at iteration 38400 to logs/fit/uones_ht/run_2/model_checkpoint_38400.h5\n",
      "13964/13964 - 1537s - loss: 0.4162 - binary_accuracy: 0.8178 - auc: 0.6428 - val_loss: 0.4455 - val_binary_accuracy: 0.8376 - val_auc: 0.6016 - 1537s/epoch - 110ms/step\n",
      "Total time for run 2 :  4835.971738100052 seconds\n",
      "Run 3 of 3\n",
      "Epoch 1/3\n",
      "INFO:tensorflow:Collective all_reduce tensors: 372 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 372 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1025s vs `on_train_batch_end` time: 0.6963s). Check your callbacks.\n",
      "Saved checkpoint at iteration 4800 to logs/fit/uones_ht/run_3/model_checkpoint_4800.h5\n",
      "Saved checkpoint at iteration 9600 to logs/fit/uones_ht/run_3/model_checkpoint_9600.h5\n",
      "13964/13964 - 1725s - loss: 0.6178 - binary_accuracy: 0.6890 - auc: 0.5534 - val_loss: 0.3953 - val_binary_accuracy: 0.8309 - val_auc: 0.6866 - 1725s/epoch - 123ms/step\n",
      "Epoch 2/3\n",
      "Saved checkpoint at iteration 14400 to logs/fit/uones_ht/run_3/model_checkpoint_14400.h5\n",
      "Saved checkpoint at iteration 19200 to logs/fit/uones_ht/run_3/model_checkpoint_19200.h5\n",
      "Saved checkpoint at iteration 24000 to logs/fit/uones_ht/run_3/model_checkpoint_24000.h5\n",
      "13964/13964 - 1546s - loss: 0.4387 - binary_accuracy: 0.8072 - auc: 0.6071 - val_loss: 0.3941 - val_binary_accuracy: 0.8388 - val_auc: 0.6522 - 1546s/epoch - 111ms/step\n",
      "Epoch 3/3\n",
      "Saved checkpoint at iteration 28800 to logs/fit/uones_ht/run_3/model_checkpoint_28800.h5\n",
      "Saved checkpoint at iteration 33600 to logs/fit/uones_ht/run_3/model_checkpoint_33600.h5\n",
      "Saved checkpoint at iteration 38400 to logs/fit/uones_ht/run_3/model_checkpoint_38400.h5\n",
      "13964/13964 - 1539s - loss: 0.4167 - binary_accuracy: 0.8176 - auc: 0.6405 - val_loss: 0.4040 - val_binary_accuracy: 0.8342 - val_auc: 0.6792 - 1539s/epoch - 110ms/step\n",
      "Total time for run 3 :  4811.577298879623 seconds\n"
     ]
    }
   ],
   "source": [
    "trained_models = []\n",
    "\n",
    "def train(num_runs, train_ds, valid_ds):\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1} of {num_runs}\")\n",
    "\n",
    "        # Clear previous session to ensure a fresh start for each run\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "        # Find the optimal number of epochs to train the model with the hyperparameters obtained from the search.\n",
    "        model = tuner.hypermodel.build(best_hps)\n",
    "        callbacks = create_callbacks(run+1)\n",
    "\n",
    "        start = time.time()\n",
    "        history = model.fit(train_ds, epochs=best_epoch, validation_data=valid_ds, batch_size=batch_size, callbacks=callbacks, verbose=2)\n",
    "        print(\"Total time for run\", run + 1, \": \", time.time() - start, \"seconds\")\n",
    "        \n",
    "        trained_models.append(model)\n",
    "\n",
    "    return trained_models\n",
    "\n",
    "# Define the number of runs\n",
    "num_runs = 3\n",
    "training = train(num_runs, train_ds, valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "692b7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(trained_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dcd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs --port 8887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55528eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs --port 8890"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbeedc",
   "metadata": {},
   "source": [
    "# Model Evaluate at Every Save Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bd4cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store checkpoint paths for each run\n",
    "checkpoint_paths_list = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    checkpoint_paths = []  # Store checkpoint paths for the current model\n",
    "\n",
    "    # Collect checkpoint paths\n",
    "    for iteration in range(4800, 38401, 4800):  # Modify this range according to your save_interval and number of checkpoints\n",
    "        checkpoint_path = f\"logs/fit/uones_ht/run_{run + 1}/model_checkpoint_{iteration}.h5\"\n",
    "        checkpoint_paths.append(checkpoint_path)\n",
    "\n",
    "    checkpoint_paths_list.append(checkpoint_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a list to store checkpoint paths for each run\n",
    "# checkpoint_paths_list = []\n",
    "\n",
    "# for model in trained_models:\n",
    "#     checkpoint_paths = []  # Store checkpoint paths for the current model\n",
    "\n",
    "#     # Collect checkpoint paths\n",
    "#     for iteration in range(4800, 52801, 4800): \n",
    "#         checkpoint_path = f\"logs/fit/uonescheckpoint/model_checkpoint_{iteration}.h5\"\n",
    "#         checkpoint_paths.append(checkpoint_path)\n",
    "\n",
    "#     checkpoint_paths_list.append(checkpoint_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f18cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Set the threshold for what messages will be logged\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf37693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec45228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a list to store predictions for each checkpoint\n",
    "# all_predictions = []\n",
    "\n",
    "# # Iterate through the collected checkpoint paths\n",
    "# for checkpoint_paths in checkpoint_paths_list:\n",
    "#     predictions = []  # Store predictions for the current run\n",
    "\n",
    "#     # Load each checkpoint and predict on the validation set\n",
    "#     for checkpoint_path in checkpoint_paths:\n",
    "#         model.load_weights(checkpoint_path)\n",
    "\n",
    "#         # Predict on the validation set\n",
    "#         checkpoint_predictions = model.predict(valid_ds)\n",
    "#         predictions.append(checkpoint_predictions)\n",
    "\n",
    "#     all_predictions.append(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f000eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_valid_images shape: (234, 320, 320, 3)\n",
      "all_valid_labels shape: (234, 14)\n",
      "8/8 [==============================] - 9s 29ms/step\n",
      "8/8 [==============================] - 0s 27ms/step\n",
      "8/8 [==============================] - 0s 27ms/step\n",
      "8/8 [==============================] - 1s 26ms/step\n",
      "8/8 [==============================] - 0s 26ms/step\n",
      "8/8 [==============================] - 0s 24ms/step\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "8/8 [==============================] - 8s 30ms/step\n",
      "8/8 [==============================] - 1s 29ms/step\n",
      "8/8 [==============================] - 0s 32ms/step\n",
      "8/8 [==============================] - 1s 33ms/step\n",
      "8/8 [==============================] - 0s 34ms/step\n",
      "8/8 [==============================] - 0s 31ms/step\n",
      "8/8 [==============================] - 1s 32ms/step\n",
      "8/8 [==============================] - 1s 35ms/step\n",
      "8/8 [==============================] - 9s 27ms/step\n",
      "8/8 [==============================] - 0s 24ms/step\n",
      "8/8 [==============================] - 1s 27ms/step\n",
      "8/8 [==============================] - 0s 28ms/step\n",
      "8/8 [==============================] - 0s 33ms/step\n",
      "8/8 [==============================] - 0s 24ms/step\n",
      "8/8 [==============================] - 1s 29ms/step\n",
      "8/8 [==============================] - 0s 25ms/step\n",
      "all pred shape: (3, 8, 234, 14)\n",
      "iteration_auroc_shape: (24, 5)\n",
      "Pathology       Average AUROC   Standard Error    95% Confidence Interval\n",
      "-----------------------------------------------------------------\n",
      "Atelectasis     0.8189          0.0070            (0.7758, 0.8621)\n",
      "Cardiomegaly    0.7804          0.0080            (0.7310, 0.8298)\n",
      "Consolidation   0.8639          0.0075            (0.8173, 0.9105)\n",
      "Edema           0.8866          0.0054            (0.8529, 0.9203)\n",
      "Pleural Effusion 0.8884          0.0051            (0.8565, 0.9203)\n",
      "\n",
      "Overall average AUROC (from top 3 models/checkpoints): 0.8477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "all_valid_images = []\n",
    "all_valid_labels = []\n",
    "\n",
    "for images, labels in valid_ds:\n",
    "    all_valid_images.append(images.numpy())\n",
    "    all_valid_labels.append(labels.numpy())\n",
    "\n",
    "# Concatenate all images and labels into two large numpy arrays\n",
    "all_valid_images = np.concatenate(all_valid_images, axis=0)\n",
    "all_valid_labels = np.concatenate(all_valid_labels, axis=0)\n",
    "print(f\"all_valid_images shape: {all_valid_images.shape}\")\n",
    "print(f\"all_valid_labels shape: {all_valid_labels.shape}\")\n",
    "\n",
    "# Initialize a list to store predictions for each checkpoint\n",
    "all_predictions = []\n",
    "\n",
    "# Iterate through the collected checkpoint paths and corresponding trained model\n",
    "for model, checkpoint_paths in zip(trained_models, checkpoint_paths_list):\n",
    "    predictions = []  # Store predictions for the current run\n",
    "\n",
    "    # Load each checkpoint and predict on the validation set\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        model.load_weights(checkpoint_path)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        checkpoint_predictions = model.predict(all_valid_images)\n",
    "        predictions.append(checkpoint_predictions)\n",
    "\n",
    "    all_predictions.append(predictions)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "print(f\"all pred shape: {all_predictions.shape}\")\n",
    "\n",
    "average_auroc_list = []\n",
    "num_pathologies = 5\n",
    "iteration_auroc = [] # List of all AUROCs per pathology\n",
    "\n",
    "for checkpoint_predictions in all_predictions:\n",
    "\n",
    "    for checkpoint_index, checkpoint_prediction in enumerate(checkpoint_predictions):\n",
    "        checkpoint_auroc_scores = []  # Store AUROC scores for the current model\n",
    "\n",
    "        for pathology_index in range(num_pathologies):\n",
    "            true_labels = all_valid_labels[:, pathology_index]\n",
    "            auroc = roc_auc_score(true_labels, checkpoint_prediction[:, pathology_index])\n",
    "            checkpoint_auroc_scores.append(auroc)\n",
    "\n",
    "        iteration_auroc.append(checkpoint_auroc_scores)\n",
    "\n",
    "# Convert iteration_auroc to array\n",
    "iteration_auroc = np.array(iteration_auroc)\n",
    "\n",
    "print(f\"iteration_auroc_shape: {iteration_auroc.shape}\")\n",
    "\n",
    "# Calculate the average AUROC across 5 pathologies\n",
    "average_auroc = np.mean(iteration_auroc, axis = 1)\n",
    "\n",
    "# Calculate the indices that would sort the average AUROC list in descending order\n",
    "sorted_indices = np.argsort(average_auroc)[::-1]\n",
    "\n",
    "# Get the top 20 indices\n",
    "top_indices = sorted_indices[:10]\n",
    "\n",
    "# Initialize a list to store the corresponding checkpoint_auroc_scores\n",
    "best_checkpoint_auroc_scores = []\n",
    "\n",
    "# Extract the checkpoint_auroc_scores for the best 20 averages\n",
    "for index in top_indices:\n",
    "    best_checkpoint_auroc_scores.append(iteration_auroc[index])\n",
    "\n",
    "best_checkpoint_auroc_scores = np.array(best_checkpoint_auroc_scores)\n",
    "\n",
    "pathology_names = ['Atelectasis','Cardiomegaly','Consolidation','Edema','Pleural Effusion']\n",
    "# Compute AUROC, Standard Deviation, and Confidence Intervals\n",
    "auroc_pathology = np.mean(best_checkpoint_auroc_scores, axis=0)\n",
    "std_dev_pathology = np.std(best_checkpoint_auroc_scores, axis=0)\n",
    "confidence_intervals = [(auroc - 1.96 * std, auroc + 1.96 * std) for auroc, std in zip(auroc_pathology, std_dev_pathology)]\n",
    "\n",
    "# Header\n",
    "print(f\"{'Pathology':<15} {'Average AUROC':<15} {'Standard Error':<17} {'95% Confidence Interval'}\")\n",
    "\n",
    "# Separator\n",
    "print('-' * 65)\n",
    "\n",
    "# Table content\n",
    "for i, pathology in enumerate(pathology_names):\n",
    "    standard_error = std_dev_pathology[i] / np.sqrt(len(best_checkpoint_auroc_scores))\n",
    "    lower_bound, upper_bound = confidence_intervals[i]\n",
    "    print(f\"{pathology:<15} {auroc_pathology[i]:<15.4f} {standard_error:<17.4f} ({lower_bound:.4f}, {upper_bound:.4f})\")\n",
    "\n",
    "# Overall AUROC\n",
    "overall_ave = np.mean(average_auroc[top_indices])\n",
    "print(f\"\\nOverall average AUROC (from top 3 models/checkpoints): {overall_ave:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c33e13e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_test_images shape: (668, 320, 320, 3)\n",
      "all_test_labels shape: (668, 14)\n",
      "21/21 [==============================] - 1s 54ms/step\n",
      "test_predictions shape: (668, 14)\n"
     ]
    }
   ],
   "source": [
    "# Getting test images and labels:\n",
    "all_test_images = []\n",
    "all_test_labels = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    all_test_images.append(images.numpy())\n",
    "    all_test_labels.append(labels.numpy())\n",
    "\n",
    "# Concatenate all images and labels into two large numpy arrays\n",
    "all_test_images = np.concatenate(all_test_images, axis=0)\n",
    "all_test_labels = np.concatenate(all_test_labels, axis=0)\n",
    "print(f\"all_test_images shape: {all_test_images.shape}\")\n",
    "print(f\"all_test_labels shape: {all_test_labels.shape}\")\n",
    "\n",
    "# Calculate the number of checkpoints per model\n",
    "num_checkpoints_per_model = len(checkpoint_paths_list[0])  # or use all_predictions.shape[1] if available\n",
    "\n",
    "# Determine the best model and checkpoint indices based on the best_model_index\n",
    "best_model_index = top_indices[0] // num_checkpoints_per_model\n",
    "best_checkpoint_index = top_indices[0] % num_checkpoints_per_model\n",
    "\n",
    "# Reference the predictions of the best model and checkpoint\n",
    "best_model_predictions = all_predictions[best_model_index, best_checkpoint_index]\n",
    "\n",
    "# Load the best checkpoint weights into the corresponding model\n",
    "best_model = trained_models[best_model_index]\n",
    "best_checkpoint_path = checkpoint_paths_list[best_model_index][best_checkpoint_index]\n",
    "best_model.load_weights(best_checkpoint_path)\n",
    "\n",
    "# Predict on the test set using the best model\n",
    "test_predictions = best_model.predict(all_test_images)\n",
    "print(f\"test_predictions shape: {test_predictions.shape}\")\n",
    "\n",
    "pathology_names = ['Atelectasis','Cardiomegaly','Consolidation','Edema','Pleural Effusion']  # \n",
    "\n",
    "# Directory where the figures will be saved\n",
    "save_dir = \"test_performance/uone_ht\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 2. Calculate and Plot ROC for each pathology\n",
    "for i in range(num_pathologies):\n",
    "    fpr, tpr, _ = roc_curve(test_labels[:, i], test_predictions[:, i])\n",
    "    auc = roc_auc_score(test_labels[:, i], test_predictions[:, i])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"{pathology_names[i]} (AUC = {auc:.2f})\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve for {pathology_names[i]}\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save ROC curve\n",
    "    roc_filename = f\"{pathology_names[i]}_auroc.png\"\n",
    "    plt.savefig(os.path.join(save_dir, roc_filename))\n",
    "    plt.close()\n",
    "\n",
    "# 2. Calculate and Plot PR curve for each pathology\n",
    "for i in range(num_pathologies):\n",
    "    precision, recall, _ = precision_recall_curve(test_labels[:, i], test_predictions[:, i])\n",
    "    average_precision = average_precision_score(test_labels[:, i], test_predictions[:, i])\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f\"{pathology_names[i]} (AP = {average_precision:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve for {pathology_names[i]}\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save PR curve\n",
    "    pr_filename = f\"{pathology_names[i]}_pr.png\"\n",
    "    plt.savefig(os.path.join(save_dir, pr_filename))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27dbcf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(trained_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2399c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
